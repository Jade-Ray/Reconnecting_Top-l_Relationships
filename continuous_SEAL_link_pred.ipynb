{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from datasets import SEALDataset, SEALDatasetInMemory, SEALPredData\n",
    "from models import DGCNN\n",
    "from utils import read_temporary_graph_data, read_graph_from_edgefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super parameters:\n",
    " - `T`: split number of the time series.\n",
    " - `N`: number of predicted continuous time series.\n",
    " - `datasets`: the graph datasets, instance of `SEALDataset`.\n",
    " - `BS`: batch size.\n",
    " - `EPOCH`: number of epochs.\n",
    " - `LEARNING_RATE`: learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1704f596310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 400\n",
    "N = 6\n",
    "datasets = ['SuperUser',]\n",
    "BS = 8192\n",
    "EPOCH = 50\n",
    "LEARN_RATE = 0.0001\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading SuperUser Dataset with T=400 + 0...\n",
      "Last temporal graph T=400 + 0 with 31 node feature size.\n",
      "\n",
      "Training SuperUser Dataset with T=400+0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:04<03:29,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [01]/[50], Loss: 0.6694, Val: 0.5032, Test: 0.4930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:06<00:10,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[50], Loss: 0.6666, Val: 0.5967, Test: 0.5771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [00:09<00:05,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21]/[50], Loss: 0.6639, Val: 0.6734, Test: 0.6580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:11<00:04,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31]/[50], Loss: 0.6615, Val: 0.6539, Test: 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:13<00:01,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41]/[50], Loss: 0.6578, Val: 0.6143, Test: 0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:15<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50]/[50], Loss: 0.6550, Val: 0.6106, Test: 0.6867\n",
      "Loss: 0.6550, Val_best: 0.6846, Test: 0.6867\n",
      "\n",
      "Predicting SuperUser Dataset with T=400 + 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load SuperUser/T400+1_pred_edge.pt to dataset\n",
      "\n",
      "Loading SuperUser Dataset with T=400 + 1...\n",
      "Last temporal graph T=400 + 1 with 57 node feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Embedding: 100%|██████████| 1/1 [00:13<00:00, 13.09s/it]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SuperUser Dataset with T=400+1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:38,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [01]/[50], Loss: 0.6991, Val: 0.5232, Test: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:04<00:14,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[50], Loss: 0.6963, Val: 0.5020, Test: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:08<00:11,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21]/[50], Loss: 0.6927, Val: 0.4850, Test: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:13<00:07,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31]/[50], Loss: 0.6891, Val: 0.4921, Test: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:17<00:03,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41]/[50], Loss: 0.6841, Val: 0.4687, Test: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:20<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50]/[50], Loss: 0.6788, Val: 0.4668, Test: 0.4762\n",
      "Loss: 0.6788, Val_best: 0.5232, Test: 0.4762\n",
      "\n",
      "Predicting SuperUser Dataset with T=400 + 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many edges predicted in SuperUser Dataset with T=400 + 2, try threshold=0.51...\n",
      "Too many edges predicted in SuperUser Dataset with T=400 + 2, try threshold=0.52...\n",
      "Too many edges predicted in SuperUser Dataset with T=400 + 2, try threshold=0.53...\n",
      "load SuperUser/T400+2_pred_edge.pt to dataset\n",
      "\n",
      "Loading SuperUser Dataset with T=400 + 2...\n",
      "Last temporal graph T=400 + 2 with 1 node feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Embedding: 100%|██████████| 1/1 [00:13<00:00, 13.08s/it]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update pred data node feature size to 58.\n",
      "\n",
      "Training SuperUser Dataset with T=400+2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:37,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [01]/[50], Loss: 0.6895, Val: 0.4922, Test: 0.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:04<00:15,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[50], Loss: 0.6864, Val: 0.4826, Test: 0.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:08<00:12,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21]/[50], Loss: 0.6835, Val: 0.4778, Test: 0.4688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:13<00:08,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31]/[50], Loss: 0.6796, Val: 0.4897, Test: 0.4755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:17<00:03,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41]/[50], Loss: 0.6755, Val: 0.4872, Test: 0.4755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:21<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50]/[50], Loss: 0.6709, Val: 0.4958, Test: 0.4755\n",
      "Loss: 0.6709, Val_best: 0.5022, Test: 0.4755\n",
      "\n",
      "Predicting SuperUser Dataset with T=400 + 3...\n",
      "load SuperUser/T400+3_pred_edge.pt to dataset\n",
      "\n",
      "Loading SuperUser Dataset with T=400 + 3...\n",
      "Last temporal graph T=400 + 3 with 3 node feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing...\n",
      "Embedding: 100%|██████████| 1/1 [00:13<00:00, 13.35s/it]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update pred data node feature size to 44.\n",
      "\n",
      "Training SuperUser Dataset with T=400+3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:43,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [01]/[50], Loss: 0.6910, Val: 0.4522, Test: 0.5217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:05<00:15,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[50], Loss: 0.6876, Val: 0.5056, Test: 0.5691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:09<00:12,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21]/[50], Loss: 0.6836, Val: 0.5207, Test: 0.5778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:13<00:08,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31]/[50], Loss: 0.6796, Val: 0.5271, Test: 0.5693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:18<00:03,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41]/[50], Loss: 0.6743, Val: 0.5295, Test: 0.5712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:21<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50]/[50], Loss: 0.6683, Val: 0.5292, Test: 0.5711\n",
      "Loss: 0.6683, Val_best: 0.5304, Test: 0.5711\n",
      "\n",
      "Predicting SuperUser Dataset with T=400 + 4...\n",
      "load SuperUser/T400+4_pred_edge.pt to dataset\n",
      "\n",
      "Loading SuperUser Dataset with T=400 + 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last temporal graph T=400 + 4 with 31 node feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Embedding: 100%|██████████| 1/1 [00:12<00:00, 12.88s/it]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update pred data node feature size to 56.\n",
      "\n",
      "Training SuperUser Dataset with T=400+4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:36,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [01]/[50], Loss: 0.7067, Val: 0.5530, Test: 0.4988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:04<00:16,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[50], Loss: 0.7016, Val: 0.5287, Test: 0.4869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:09<00:11,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21]/[50], Loss: 0.6971, Val: 0.4776, Test: 0.4869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:13<00:07,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31]/[50], Loss: 0.6927, Val: 0.4662, Test: 0.4869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:17<00:04,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41]/[50], Loss: 0.6873, Val: 0.4644, Test: 0.4869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:21<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50]/[50], Loss: 0.6824, Val: 0.4658, Test: 0.4869\n",
      "Loss: 0.6824, Val_best: 0.5564, Test: 0.4869\n",
      "\n",
      "Predicting SuperUser Dataset with T=400 + 5...\n",
      "load SuperUser/T400+5_pred_edge.pt to dataset\n",
      "\n",
      "Loading SuperUser Dataset with T=400 + 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last temporal graph T=400 + 5 with 90 node feature size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Embedding: 100%|██████████| 1/1 [00:12<00:00, 12.88s/it]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SuperUser Dataset with T=400+5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:44,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [01]/[50], Loss: 0.7051, Val: 0.4381, Test: 0.4321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:05<00:17,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [11]/[50], Loss: 0.6999, Val: 0.4592, Test: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:09<00:13,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [21]/[50], Loss: 0.6953, Val: 0.4906, Test: 0.4862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:14<00:08,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31]/[50], Loss: 0.6901, Val: 0.4985, Test: 0.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:18<00:03,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [41]/[50], Loss: 0.6838, Val: 0.5037, Test: 0.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:22<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50]/[50], Loss: 0.6773, Val: 0.5081, Test: 0.5021\n",
      "Loss: 0.6773, Val_best: 0.5081, Test: 0.5021\n",
      "\n",
      "Predicting SuperUser Dataset with T=400 + 6...\n",
      "load SuperUser/T400+6_pred_edge.pt to dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, criterion, data_loader, device='cuda'):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in data_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data_loader, device='cuda'):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in data_loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, data_loader, threshold=0.5, device='cuda'):\n",
    "    pred_edge_index = torch.zeros(0, 2).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for j, data in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        logits = logits.view(-1).sigmoid()\n",
    "        batch_mask = (logits > threshold).nonzero(as_tuple=True)[0]\n",
    "        pred_edge_index = torch.cat((pred_edge_index, data.pred_edge[batch_mask]), dim=0)\n",
    "    \n",
    "    return pred_edge_index.type(torch.int).cpu()\n",
    "\n",
    "\n",
    "def build_dataloader(dataset_name: str, t: int, additional_graphs=None, pred_from_add=True):\n",
    "    n = 0 if additional_graphs is None else len(additional_graphs)\n",
    "    print(f'\\nLoading {dataset_name} Dataset with T={t} + {n}...')\n",
    "    \n",
    "    if n > 0 and pred_from_add:\n",
    "        pred_graph = additional_graphs[-1]\n",
    "    else:\n",
    "        pred_graph = read_temporary_graph_data(\n",
    "            f'data/SEALDataset/{dataset_name}/raw/{SEALDataset.info[dataset_name][\"file\"]}', \n",
    "            SEALDataset.info[dataset_name][\"timespan\"], t)[-1]\n",
    "    assert pred_graph.number_of_nodes() > 30, f'The last temporal graph of {dataset_name} Dataset with T={t} is too small, consider to decrease T.'\n",
    "    \n",
    "    if dataset_name in ['WikiTalk', 'StackOverflow', 'SuperUser']:\n",
    "        # For large dataset, we use SEALDataset just load last temporal graph data.\n",
    "        # Cause splited data from last temporal graph's node feature size is much smaller than the whole last temporal graph.\n",
    "        # So we should first load the whole last temporal graph data to get the max node feature size.\n",
    "        pred_pos_data, pred_neg_data = SEALPredData.get_pos_neg_data(pred_graph)\n",
    "        pred_data_list = SEALPredData.toSEAL_pred_datalist(pred_pos_data, pred_neg_data)\n",
    "        print(f'Last temporal graph T={t} + {n} with {SEALPredData._max_z} node feature size.')\n",
    "        \n",
    "        # change the pred_idx range to choose the best training data.\n",
    "        params = {'pred_idx': slice(-1, None), 'num_hops': 2, 'T': t, 'max_z': SEALPredData._max_z, 'additional_graphs': additional_graphs}\n",
    "        train_dataset = SEALDataset('data/SEALDataset', dataset_name, 'train', **params)\n",
    "        val_dataset = SEALDataset('data/SEALDataset', dataset_name, 'val', **params)\n",
    "        test_dataset = SEALDataset('data/SEALDataset', dataset_name, 'test', **params)\n",
    "        # Avoid splited data have more node feature size than the whole last temporal graph.\n",
    "        if SEALPredData._max_z < train_dataset.num_features-1:\n",
    "            pred_data_list = SEALPredData.toSEAL_pred_datalist(pred_pos_data, pred_neg_data, num_features=train_dataset.num_features)\n",
    "            print(f'Update pred data node feature size to {train_dataset.num_features}.')\n",
    "    else:\n",
    "        # For small dataset, we use SEALDatasetInMemory to load all temporal graph data.\n",
    "        params = {'num_hops': 2, 'T': t, 'additional_graphs': additional_graphs}\n",
    "        train_dataset = SEALDatasetInMemory('data/SEALDataset', dataset_name, 'train', **params)\n",
    "        val_dataset = SEALDatasetInMemory('data/SEALDataset', dataset_name, 'val', **params)\n",
    "        test_dataset = SEALDatasetInMemory('data/SEALDataset', dataset_name, 'test', **params)\n",
    "        # Obviously, the whole temporal graph's node feature size with the max node feature size.\n",
    "        # So we use this num_features as the pred_data_list's node feature size.\n",
    "        pred_pos_data, pred_neg_data = SEALPredData.get_pos_neg_data(pred_graph)\n",
    "        pred_data_list = SEALPredData.toSEAL_pred_datalist(pred_pos_data, pred_neg_data, num_features=train_dataset.num_features)\n",
    "        print(f'Last temporal graph T={t} + {n} with {SEALPredData._max_z} node feature size.')\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BS)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BS)\n",
    "    pred_loader = DataLoader(pred_data_list, batch_size=BS)\n",
    "    return train_loader, val_loader, test_loader, pred_loader\n",
    "\n",
    "\n",
    "def build_model(train_dataset, hidden_channels=32, num_layers=3):\n",
    "    model = DGCNN(hidden_channels=hidden_channels, num_layers=num_layers, train_dataset=train_dataset).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARN_RATE)\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    return model, optimizer, criterion\n",
    "    \n",
    "\n",
    "def load_predict_graph(file_name: str):\n",
    "    graph = read_graph_from_edgefile(file_name)\n",
    "    # remove isolate node and self loop\n",
    "    graph.remove_nodes_from(nx.isolates(graph))\n",
    "    graph.remove_edges_from(nx.selfloop_edges(graph))\n",
    "    return graph\n",
    "\n",
    "\n",
    "def is_pred_edge_num_too_small(pred_edge_index, threshold=40):\n",
    "    DG = nx.DiGraph()\n",
    "    DG.add_edges_from(pred_edge_index.numpy())\n",
    "    DG.remove_nodes_from(list(nx.isolates(DG)))\n",
    "    DG.remove_edges_from(nx.selfloop_edges(DG))\n",
    "    return DG.number_of_edges() < threshold\n",
    "\n",
    "\n",
    "def is_pred_edge_num_too_many(pred_edge_index, threshold=7000):\n",
    "    DG = nx.DiGraph()\n",
    "    DG.add_edges_from(pred_edge_index.numpy())\n",
    "    DG.remove_nodes_from(list(nx.isolates(DG)))\n",
    "    DG.remove_edges_from(nx.selfloop_edges(DG))\n",
    "    return DG.number_of_edges() > threshold\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    predict_graphs = []\n",
    "    t = T\n",
    "    for n in range(1, N+1):\n",
    "        # If current graph has been predicted, skip it.\n",
    "        if osp.exists(f'data/SEALDataset/{dataset}/T{t}+{n}_pred_edge.pt'):\n",
    "            print(f'{dataset}/T{t}+{n} has been predicted, just load.')\n",
    "            predict_graphs.append(load_predict_graph(f'data/SEALDataset/{dataset}/T{t}+{n}_pred_edge.pt'))\n",
    "            continue\n",
    "        \n",
    "        train_loader, val_loader, test_loader, pred_loader = build_dataloader(dataset, T, additional_graphs=predict_graphs)\n",
    "        model, optimizer, criterion = build_model(train_loader.dataset)\n",
    "        \n",
    "        if osp.exists(f'data/models/SEAL_{dataset}_T{t}+{n-1}.pth'):\n",
    "            model.load_state_dict(torch.load(f'data/models/SEAL_{dataset}_T{t}+{n-1}.pth'))\n",
    "        else:\n",
    "            print(f'\\nTraining {dataset} Dataset with T={t}+{n-1}...')\n",
    "            best_val_auc = test_auc = 0\n",
    "            for epoch in tqdm(range(EPOCH)):\n",
    "                loss = train(model, optimizer, criterion, train_loader)\n",
    "                val_auc = test(model, val_loader)\n",
    "                if val_auc > best_val_auc:\n",
    "                    best_val_auc = val_auc\n",
    "                    test_auc = test(model, test_loader)\n",
    "                if epoch % 10 == 0 or epoch == EPOCH-1:\n",
    "                    print(f'Epoch: [{epoch+1:02d}]/[{EPOCH:02d}], Loss: {loss:.4f}, Val: {val_auc:.4f}, Test: {test_auc:.4f}')\n",
    "            print(f'Loss: {loss:.4f}, Val_best: {best_val_auc:.4f}, Test: {test_auc:.4f}')\n",
    "            \n",
    "            torch.save(model.state_dict(), f'data/models/SEAL_{dataset}_T{T}+{n-1}.pth')\n",
    "        \n",
    "        print(f'\\nPredicting {dataset} Dataset with T={t} + {n}...')\n",
    "        pred_edge_index = predict(model, pred_loader, threshold=0.5)\n",
    "        if is_pred_edge_num_too_small(pred_edge_index):\n",
    "            # Avoid no edge predicted, we will descend the threshold by 0.01 until we get some edges.\n",
    "            for threshold in range(49, 0, -1):\n",
    "                print(f'No edge predicted in {dataset} Dataset with T={t} + {n}, try threshold={threshold/100}...')\n",
    "                pred_edge_index = predict(model, pred_loader, threshold=threshold/100)\n",
    "                if not is_pred_edge_num_too_small(pred_edge_index):\n",
    "                    break\n",
    "        assert not is_pred_edge_num_too_small(pred_edge_index), f'No edge predicted in {dataset} Dataset with T={t} + {n}...'\n",
    "        if is_pred_edge_num_too_many(pred_edge_index):\n",
    "            # Avoid too many edges predicted, we will ascend the threshold by 0.01 until we get less edges.\n",
    "            for threshold in range(51, 100, 1):\n",
    "                print(f'Too many edges predicted in {dataset} Dataset with T={t} + {n}, try threshold={threshold/100}...')\n",
    "                pred_edge_index = predict(model, pred_loader, threshold=threshold/100)\n",
    "                if not is_pred_edge_num_too_many(pred_edge_index):\n",
    "                    break\n",
    "        assert not is_pred_edge_num_too_many(pred_edge_index), f'Too many edges predicted in {dataset} Dataset with T={t} + {n}...'\n",
    "        torch.save(pred_edge_index, f'data/SEALDataset/{dataset}/T{t}+{n}_pred_edge.pt')\n",
    "        # load current pred graph to predict_graphs\n",
    "        print(f'load {dataset}/T{t}+{n}_pred_edge.pt to dataset')\n",
    "        predict_graphs.append(load_predict_graph(f'data/SEALDataset/{dataset}/T{t}+{n}_pred_edge.pt'))\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<networkx.classes.digraph.DiGraph at 0x1700c5d6640>,\n",
       " <networkx.classes.digraph.DiGraph at 0x1704dd1bd30>,\n",
       " <networkx.classes.digraph.DiGraph at 0x17017d679a0>,\n",
       " <networkx.classes.digraph.DiGraph at 0x170fe7680a0>,\n",
       " <networkx.classes.digraph.DiGraph at 0x1725a368130>,\n",
       " <networkx.classes.digraph.DiGraph at 0x170fe769100>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for g in predict_graphs:    \n",
    "    options = {\n",
    "        'node_color': '#1f78b4',\n",
    "        'node_size': 500,\n",
    "        'width': 3,\n",
    "        'arrowstyle': '-|>',\n",
    "        'arrowsize': 12,\n",
    "    }\n",
    "    ax = plt.figure(figsize=(10, 10), dpi=100)\n",
    "    nx.draw_networkx(g, pos=nx.spring_layout(g), **options)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyGeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
